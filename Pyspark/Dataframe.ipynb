{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Dataframe_session_name').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframe_session_name</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x287ee231b80>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv('BasicDataframe.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+----------+\n",
      "|    _c0|_c1|   _c2|       _c3|       _c4|\n",
      "+-------+---+------+----------+----------+\n",
      "|   Name|Age|Salary|Department|Experience|\n",
      "|  Pawan| 26| 50000| Furniture|         4|\n",
      "|Chandar| 55| 70000| Furniture|        20|\n",
      "| Sanjay| 22| 35000|Operations|         1|\n",
      "|  Anand| 25| 20000| Furniture|         1|\n",
      "+-------+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Making top row as header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+----------+\n",
      "|   Name|Age|Salary|Department|Experience|\n",
      "+-------+---+------+----------+----------+\n",
      "|  Pawan| 26| 50000| Furniture|         4|\n",
      "|Chandar| 55| 70000| Furniture|        20|\n",
      "| Sanjay| 22| 35000|Operations|         1|\n",
      "|  Anand| 25| 20000| Furniture|         1|\n",
      "+-------+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('header','true').csv('BasicDataframe.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+----------+\n",
      "|   Name|Age|Salary|Department|Experience|\n",
      "+-------+---+------+----------+----------+\n",
      "|  Pawan| 26| 50000| Furniture|         4|\n",
      "|Chandar| 55| 70000| Furniture|        20|\n",
      "| Sanjay| 22| 35000|Operations|         1|\n",
      "|  Anand| 25| 20000| Furniture|         1|\n",
      "+-------+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn('Salary', col('Salary').cast('integer'))\n",
    "df = df.withColumn('Age', col('Age').cast('integer'))\n",
    "df = df.withColumn('Experience', col('Experience').cast('integer'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+----------+\n",
      "|   Name|Age|Salary|Department|Experience|\n",
      "+-------+---+------+----------+----------+\n",
      "|  Pawan| 26| 50000| Furniture|       4.0|\n",
      "|Chandar| 55| 70000| Furniture|      20.0|\n",
      "| Sanjay| 22| 35000|Operations|       1.0|\n",
      "|  Anand| 25| 20000| Furniture|       1.0|\n",
      "+-------+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(header=True, path='BasicDataframe.csv', inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Experience: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Name='Pawan', Age=26, Salary=50000, Department='Furniture', Experience=4.0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Pawan', Age=26, Salary=50000, Department='Furniture', Experience=4.0),\n",
       " Row(Name='Chandar', Age=55, Salary=70000, Department='Furniture', Experience=20.0),\n",
       " Row(Name='Sanjay', Age=22, Salary=35000, Department='Operations', Experience=1.0)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Sanjay', Age=22, Salary=35000, Department='Operations', Experience=1.0),\n",
       " Row(Name='Anand', Age=25, Salary=20000, Department='Furniture', Experience=1.0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [Name#410,Age#411,Salary#412,Department#413,Experience#414] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/d:/DEVELOPMENT/Big Data/Pyspark/BasicDataframe.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Name:string,Age:int,Salary:int,Department:string,Experience:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Experience: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Salary', 'Department', 'Experience']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Name|\n",
      "+-------+\n",
      "|  Pawan|\n",
      "|Chandar|\n",
      "| Sanjay|\n",
      "|  Anand|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Name').show()  # dataframe type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Name|\n",
      "+-------+\n",
      "|Chandar|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Name').where(df['Salary']>50000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('Name').where(df['Salary']>50000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Pawan| 26|\n",
      "|Chandar| 55|\n",
      "| Sanjay| 22|\n",
      "|  Anand| 25|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Name', 'Age']).show()  # dataframe type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('Age', 'int'),\n",
       " ('Salary', 'int'),\n",
       " ('Department', 'string'),\n",
       " ('Experience', 'double')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Name: string, Age: string, Salary: string, Department: string, Experience: string]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+-----------------+----------+---------------+\n",
      "|summary|  Name|               Age|           Salary|Department|     Experience|\n",
      "+-------+------+------------------+-----------------+----------+---------------+\n",
      "|  count|     4|                 4|                4|         4|              4|\n",
      "|   mean|  NULL|              32.0|          43750.0|      NULL|            6.5|\n",
      "| stddev|  NULL|15.427248620541512|21360.00936329383|      NULL|9.1104335791443|\n",
      "|    min| Anand|                22|            20000| Furniture|            1.0|\n",
      "|    max|Sanjay|                55|            70000|Operations|           20.0|\n",
      "+-------+------+------------------+-----------------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CRUD columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+----------+-------------+\n",
      "|   Name|Age|Salary|Department|Experience|5 years later|\n",
      "+-------+---+------+----------+----------+-------------+\n",
      "|  Pawan| 26| 50000| Furniture|       4.0|           31|\n",
      "|Chandar| 55| 70000| Furniture|      20.0|           60|\n",
      "| Sanjay| 22| 35000|Operations|       1.0|           27|\n",
      "|  Anand| 25| 20000| Furniture|       1.0|           30|\n",
      "+-------+---+------+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('5 years later', df['Age']+5)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+----------+-----+\n",
      "|   Name|Age|Salary|Department|Experience|Age+5|\n",
      "+-------+---+------+----------+----------+-----+\n",
      "|  Pawan| 26| 50000| Furniture|       4.0|   31|\n",
      "|Chandar| 55| 70000| Furniture|      20.0|   60|\n",
      "| Sanjay| 22| 35000|Operations|       1.0|   27|\n",
      "|  Anand| 25| 20000| Furniture|       1.0|   30|\n",
      "+-------+---+------+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('5 years later', 'Age+5')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+----------+\n",
      "|   Name|Age|Salary|Department|Experience|\n",
      "+-------+---+------+----------+----------+\n",
      "|  Pawan| 26| 50000| Furniture|       4.0|\n",
      "|Chandar| 55| 70000| Furniture|      20.0|\n",
      "| Sanjay| 22| 35000|Operations|       1.0|\n",
      "|  Anand| 25| 20000| Furniture|       1.0|\n",
      "+-------+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('Age+5')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+----------+\n",
      "|   Name|Age|Salary|Department|Experience|\n",
      "+-------+---+------+----------+----------+\n",
      "|  Pawan| 26| 50000| Furniture|       4.0|\n",
      "|Chandar| 55| 70000| Furniture|      20.0|\n",
      "| Sanjay| 22| 35000|Operations|       1.0|\n",
      "|  Anand| 25| 20000| Furniture|       1.0|\n",
      "+-------+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+----------+----------+\n",
      "|  Name|Age|Salary|Department|Experience|\n",
      "+------+---+------+----------+----------+\n",
      "|Sanjay| 22| 35000|Operations|       1.0|\n",
      "| Anand| 25| 20000| Furniture|       1.0|\n",
      "+------+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Salary<=40000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Sanjay| 22|\n",
      "| Anand| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Salary<=40000').select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Sanjay| 22|\n",
      "| Anand| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Salary']<=40000).select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Pawan| 26|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Salary']>40000) & (df['Salary']<60000)).select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Chandar| 55|\n",
      "| Sanjay| 22|\n",
      "|  Anand| 25|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(~((df['Salary']>40000) & (df['Salary']<60000))).select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Chandar| 55|\n",
      "| Sanjay| 22|\n",
      "|  Anand| 25|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Salary']<=40000) | (df['Salary']>60000)).select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by and Agg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+----------+\n",
      "|   Name|Age|Salary|Department|Experience|\n",
      "+-------+---+------+----------+----------+\n",
      "|  Pawan| 26| 50000| Furniture|       4.0|\n",
      "|Chandar| 55| 70000| Furniture|      20.0|\n",
      "| Sanjay| 22| 35000|Operations|       1.0|\n",
      "|  Anand| 25| 20000| Furniture|       1.0|\n",
      "+-------+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.group.GroupedData"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.groupBy('Department'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|Department|count|\n",
      "+----------+-----+\n",
      "| Furniture|    3|\n",
      "|Operations|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|Department|Experience|count|\n",
      "+----------+----------+-----+\n",
      "| Furniture|       1.0|    1|\n",
      "| Furniture|      20.0|    1|\n",
      "| Furniture|       4.0|    1|\n",
      "|Operations|       1.0|    1|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Department', 'Experience').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|min(Salary)|\n",
      "+----------+-----------+\n",
      "| Furniture|      20000|\n",
      "|Operations|      35000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Department').min('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+-----------------+\n",
      "|Department|          avg(Age)|       avg(Salary)|  avg(Experience)|\n",
      "+----------+------------------+------------------+-----------------+\n",
      "| Furniture|35.333333333333336|46666.666666666664|8.333333333333334|\n",
      "|Operations|              22.0|           35000.0|              1.0|\n",
      "+----------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Department').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|max(Salary)|\n",
      "+----------+-----------+\n",
      "| Furniture|      70000|\n",
      "|Operations|      35000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Department').max('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|sum(Salary)|\n",
      "+----------+-----------+\n",
      "| Furniture|     140000|\n",
      "|Operations|      35000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Department').sum('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SparkSession.__init__() got an unexpected keyword argument 'conf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf\n\u001b[0;32m      3\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMap_and_filter\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m n \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m      7\u001b[0m rdd \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mparallelize(n)\n",
      "\u001b[1;31mTypeError\u001b[0m: SparkSession.__init__() got an unexpected keyword argument 'conf'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "conf = SparkConf().setAppName('Map_and_filter').setMaster(\"local[*]\")\n",
    "spark = SparkSession(conf = conf)\n",
    "\n",
    "n = [1,2,3,4,5]\n",
    "rdd = spark.parallelize(n)\n",
    "\n",
    "sq_rdd = rdd.map(lambda x: x * x)\n",
    "\n",
    "filtered_rdd = sq_rdd.filter(lambda x: x > 16)\n",
    "\n",
    "res = filtered_rdd.collect()\n",
    "print(res)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Dataframe_session_name, master=local[*]) created by getOrCreate at C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_12400\\632769566.py:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf, SparkContext\n\u001b[0;32m      3\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMapFilter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m      7\u001b[0m rdd \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize(data)\n",
      "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    454\u001b[0m             currentAppName,\n\u001b[0;32m    455\u001b[0m             currentMaster,\n\u001b[0;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[0;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[0;32m    459\u001b[0m         )\n\u001b[0;32m    460\u001b[0m     )\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Dataframe_session_name, master=local[*]) created by getOrCreate at C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_12400\\632769566.py:2 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"MapFilter\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "print(mapped_rdd.collect())  # Output: [2, 4, 6, 8, 10]\n",
    "\n",
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(filtered_rdd.collect())  # Output: [2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
